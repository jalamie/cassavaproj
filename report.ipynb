{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[train]_images the image files. Number of images: 21397\n",
    "\n",
    "train.csv\n",
    "\n",
    "- image_id the image file name.\n",
    "- label the ID code for the disease.\n",
    "\n",
    "sample_submission.csv A properly formatted sample submission, given the disclosed test set content.\n",
    "\n",
    "- image_id the image file name.\n",
    "- label the predicted ID code for the disease.\n",
    "[train/test]_tfrecords the image files in tfrecord format. (not used)\n",
    "\n",
    "label_num_to_disease_map.json The mapping between each disease code and the real disease name."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to split the data into two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to your data root directory\n",
    "data_root = './data'\n",
    "\n",
    "# Set the path to the directory for resized images\n",
    "resized_dir = './resized_data_split_less'\n",
    "\n",
    "# Set the desired size for the resized images\n",
    "target_size = (150, 150)\n",
    "\n",
    "# Set the test size for the train-test split\n",
    "test_size = 0.2\n",
    "\n",
    "# Create the directory for resized images if it doesn't exist\n",
    "os.makedirs(resized_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each subdirectory (class)\n",
    "for illness_class in os.listdir(data_root):\n",
    "    class_path = os.path.join(data_root, illness_class)\n",
    "    \n",
    "    # Skip non-directory files\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    # Get the list of images in the current class\n",
    "    images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "\n",
    "    # Split the images into training and testing sets\n",
    "    train_images, test_images = train_test_split(images, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Resize and save each training image to the new directory\n",
    "    for img in train_images:\n",
    "        src_path = os.path.join(class_path, img)\n",
    "        dest_path = os.path.join(resized_dir, 'train', illness_class, img)\n",
    "        os.makedirs(os.path.join(resized_dir, 'train', illness_class), exist_ok=True)\n",
    "\n",
    "        # Open and resize the image\n",
    "        with Image.open(src_path) as image:\n",
    "            resized_image = image.resize(target_size)\n",
    "            resized_image.save(dest_path)\n",
    "\n",
    "    # Resize and save each testing image to the new directory\n",
    "    for img in test_images:\n",
    "        src_path = os.path.join(class_path, img)\n",
    "        dest_path = os.path.join(resized_dir, 'test', illness_class, img)\n",
    "        os.makedirs(os.path.join(resized_dir, 'test', illness_class), exist_ok=True)\n",
    "\n",
    "        # Open and resize the image\n",
    "        with Image.open(src_path) as image:\n",
    "            resized_image = image.resize(target_size)\n",
    "            resized_image.save(dest_path)\n",
    "\n",
    "print(\"Images resized and saved to the new directory with train-test split.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the code below to format various types of data to test with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and save each training image to the new directory\n",
    "    for img in train_images:\n",
    "        src_path = os.path.join(train_class_path, img)\n",
    "        dest_path = os.path.join(gray_dir, 'train', illness_class, img)\n",
    "\n",
    "        # Open and convert the image to grayscale\n",
    "        with Image.open(src_path) as image:\n",
    "            gray_image = image.convert('L')\n",
    "            gray_image.save(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def sharpen_image(image):\n",
    "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n",
    "    return image_sharp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load all library needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import pathlib\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./resized_data_split/train'\n",
    "test_path='./resized_data_split/test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the batch size for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader_32\u001b[39m=\u001b[39mDataLoader(\n\u001b[1;32m      2\u001b[0m     torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mImageFolder(train_path,transform\u001b[39m=\u001b[39mtransformer),\n\u001b[1;32m      3\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m test_loader_32\u001b[39m=\u001b[39mDataLoader(\n\u001b[1;32m      6\u001b[0m     torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mImageFolder(test_path,transform\u001b[39m=\u001b[39mtransformer),\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader_32=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "test_loader_32=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=5):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (256,20,75,75)\n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "        #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare numbner of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the type of model to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=5).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimiser to be used, set learning rate and weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model based on above parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best accuracy variable\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate through each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize training accuracy and loss\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Iterate through the training dataset\n",
    "    for i, (images, labels) in enumerate(train_loader_32):\n",
    "        # Move images and labels to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update training loss\n",
    "        train_loss += loss.cpu().data * images.size(0)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    # Calculate average training accuracy and loss\n",
    "    train_accuracy = train_accuracy / train_count\n",
    "    train_loss = train_loss / train_count\n",
    "    # Initialize the confusion matrix\n",
    "    conf_matrix = torch.zeros(5, 5)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize testing accuracy\n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    # Iterate through the testing dataset\n",
    "    for i, (images, labels) in enumerate(test_loader_32):\n",
    "        # Move images and labels to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate testing accuracy\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        # Update the confusion matrix\n",
    "        conf_matrix += confusion_matrix(labels.cpu().numpy(), prediction.cpu().numpy(), labels=list(range(5)))\n",
    "    \n",
    "    # Calculate average testing accuracy\n",
    "    test_accuracy = test_accuracy / test_count\n",
    "    class_accuracy = conf_matrix.diag() / conf_matrix.sum(1)\n",
    "\n",
    "    # Print class-wise accuracy\n",
    "    for i in range(5):\n",
    "        print(f'Class {i} Accuracy: {class_accuracy[i].item()}')\n",
    "        # Print epoch-wise results\n",
    "    print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) +\n",
    "        ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
    "    \n",
    "    # Save the best model based on testing accuracy\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a different model that includes dropout probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_dropout(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_prob=0.5):\n",
    "        super(ConvNet_dropout, self).__init__()\n",
    "        \n",
    "        # Output size after convolution filter ((w-f+2P)/s) + 1\n",
    "        \n",
    "        # Input shape = (256, 3, 150, 150)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Reduce the image size by factor 2\n",
    "        # Shape = (256, 12, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 20, 75, 75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Shape = (256, 20, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=75 * 75 * 32, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "            \n",
    "        output = self.pool(output)\n",
    "        output = self.dropout1(output)\n",
    "            \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.dropout2(output)\n",
    "            \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.dropout3(output)\n",
    "            \n",
    "        # Above output will be in matrix form, with shape (256, 32, 75, 75)\n",
    "        output = output.view(-1, 32 * 75 * 75)\n",
    "            \n",
    "        output = self.fc(output)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the new model would be set with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = ConvNet_dropout(num_classes=5, dropout_prob=0.1).to(device)\n",
    "optimizer_dropout=Adam(model_dropout.parameters(),lr=0.001,weight_decay=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new data can be loaded for testing like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for training and testing directory\n",
    "train_path='./resized_data_split_less/train'\n",
    "test_path='./resized_data_split_less/test'\n",
    "\n",
    "#set the batch size for CNN\n",
    "train_loader_32=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "test_loader_32=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model for tanh where relu is replaced by tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_dropout_tanh(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_prob=0.5):\n",
    "        super(ConvNet_dropout_tanh, self).__init__()\n",
    "        \n",
    "        # Output size after convolution filter ((w-f+2P)/s) + 1\n",
    "        \n",
    "        # Input shape = (256, 3, 150, 150)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Reduce the image size by factor 2\n",
    "        # Shape = (256, 12, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 20, 75, 75)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        # Shape = (256, 20, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=75 * 75 * 32, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.tanh1(output)\n",
    "            \n",
    "        output = self.pool(output)\n",
    "        output = self.dropout1(output)\n",
    "            \n",
    "        output = self.conv2(output)\n",
    "        output = self.tanh2(output)\n",
    "        output = self.dropout2(output)\n",
    "            \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.tanh3(output)\n",
    "        output = self.dropout3(output)\n",
    "            \n",
    "        # Above output will be in matrix form, with shape (256, 32, 75, 75)\n",
    "        output = output.view(-1, 32 * 75 * 75)\n",
    "            \n",
    "        output = self.fc(output)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pooling function is set to avg pooling in the new model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_dropout_avgpool(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_prob=0.5):\n",
    "        super(ConvNet_dropout_avgpool, self).__init__()\n",
    "        \n",
    "        # Output size after convolution filter ((w-f+2P)/s) + 1\n",
    "        \n",
    "        # Input shape = (256, 3, 150, 150)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Shape = (256, 12, 150, 150)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "        # Reduce the image size by factor 2\n",
    "        # Shape = (256, 12, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 20, 75, 75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Shape = (256, 20, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Shape = (256, 32, 75, 75)\n",
    "        \n",
    "        # Dropout layer with specified dropout probability\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=75 * 75 * 32, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "            \n",
    "        output = self.pool(output)\n",
    "        output = self.dropout1(output)\n",
    "            \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.dropout2(output)\n",
    "            \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.dropout3(output)\n",
    "            \n",
    "        # Above output will be in matrix form, with shape (256, 32, 75, 75)\n",
    "        output = output.view(-1, 32 * 75 * 75)\n",
    "            \n",
    "        output = self.fc(output)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
